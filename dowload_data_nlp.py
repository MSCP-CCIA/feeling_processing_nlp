# -*- coding: utf-8 -*-
"""Dowload Data - NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gsYahX4cC82TvP5n26SErJKrCQKa9gRt
"""

!pip -q install -U datasets emoji spacy
!python -m spacy download en_core_web_sm

import os
import zipfile
import requests
from datasets import load_dataset, load_from_disk
import re
import emoji
import spacy

URL = "https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
ZIP_PATH = "sentiment140.zip"
OUT_DIR = "sentiment140"

if not os.path.exists(ZIP_PATH):
    print("Descargando el archivo ZIP...")
    r = requests.get(URL, timeout=120)
    r.raise_for_status()
    open(ZIP_PATH, "wb").write(r.content)
    print("Descarga completa.")

os.makedirs(OUT_DIR, exist_ok=True)
with zipfile.ZipFile(ZIP_PATH) as zf:
    print(f"Extrayendo archivos a '{OUT_DIR}'...")
    zf.extractall(OUT_DIR)
    print("Extracci√≥n completa.")

train_csv = os.path.join(OUT_DIR, "training.1600000.processed.noemoticon.csv")
test_csv = os.path.join(OUT_DIR, "testdata.manual.2009.06.14.csv")
column_names = ["sentiment", "id", "date", "query", "user", "text"]

print("Cargando el dataset desde los archivos CSV...")
ds = load_dataset(
    "csv",
    data_files={"train": train_csv, "test": test_csv},
    column_names=column_names,
    encoding="ISO-8859-1"
)
print("Dataset cargado:", ds)

print("Filtrando y limpiando el dataset...")
ds_bin = ds.filter(lambda x: x["sentiment"] != 2)
ds_bin = ds_bin.map(lambda x: {"label": 1 if x["sentiment"] == 4 else 0})
ds_bin = ds_bin.remove_columns(["sentiment"])
ds_bin = ds_bin.remove_columns(["id", "date", "query", "user"])
print("Dataset binario y limpio:", ds_bin)

OUTPUT_DIR_PARQUET = "sentiment140_binary_dataset"
print(f"Guardando el dataset en formato Parquet en la carpeta '{OUTPUT_DIR_PARQUET}'...")
ds_bin.save_to_disk(OUTPUT_DIR_PARQUET)
print("Dataset guardado exitosamente.")
print(f"Ahora puedes cargar el dataset directamente con: load_from_disk('{OUTPUT_DIR_PARQUET}')")

OUTPUT_DIR_PARQUET = "sentiment140_binary_dataset"

if os.path.exists(OUTPUT_DIR_PARQUET):
    print("Cargando el dataset desde el disco...")
    loaded_dataset = load_from_disk(OUTPUT_DIR_PARQUET)
    print("Dataset cargado exitosamente.")
    print(loaded_dataset)
else:
    print(f"Error: La carpeta '{OUTPUT_DIR_PARQUET}' no fue encontrada.")

nlp = spacy.load("en_core_web_sm")

def preprocess_text_batch(texts: list,
                          lemmatize: bool = False,
                          remove_stopwords: bool = True,
                          handle_emojis: str = 'demojize',
                          remove_punctuation: bool = True,
                          normalize_lengthening: bool = True):

    processed_texts_for_spacy = []
    for text in texts:
        text = text.lower()
        text = re.sub(r'http\S+|@\w+', '', text)
        if normalize_lengthening:
            text = re.sub(r'(.)\1{2,}', r'\1', text)
        if handle_emojis == 'demojize':
            text = emoji.demojize(text, delimiters=(" ", " "))
        elif handle_emojis == 'drop':
            text = emoji.demojize(text, delimiters=("", ""))
            text = re.sub(r':\S+:', '', text)
        processed_texts_for_spacy.append(text)

    final_processed_texts = []
    for doc in nlp.pipe(processed_texts_for_spacy, n_process=-1, batch_size=1000):
        processed_tokens = []
        for token in doc:
            if remove_punctuation and token.is_punct:
                continue
            if remove_stopwords and token.is_stop:
                continue
            token_text = token.lemma_ if lemmatize else token.text
            if token_text.strip():
                processed_tokens.append(token_text)
        final_processed_texts.append(" ".join(processed_tokens))

    return final_processed_texts
def apply_preprocessing_batch(examples):
    examples["text"] = preprocess_text_batch(
        examples["text"],
        lemmatize=True,
        remove_stopwords=True,
        remove_punctuation=True
    )
    return examples

processed_dataset = ds_bin.map(
    apply_preprocessing_batch,
    batched=True,
    num_proc=4
)

print(processed_dataset[0]['text'])

print(processed_dataset['train'][0])

print(ds_bin['train'][0])

processed_dataset.save_to_disk("processed_dataset.parquet")